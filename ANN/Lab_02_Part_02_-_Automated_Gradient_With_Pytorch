{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = tensor([[2.0], [4.0], [6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0564],\n",
       "        [-0.9889],\n",
       "        [-1.9213]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1]) tensor([[-0.9324]])\n",
      "torch.Size([1]) tensor([0.8760])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape,p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 29.709991455078125 \n",
      "Epoch: 1 | Loss: 13.290584564208984 \n",
      "Epoch: 2 | Loss: 5.980203151702881 \n",
      "Epoch: 3 | Loss: 2.72491455078125 \n",
      "Epoch: 4 | Loss: 1.2748510837554932 \n",
      "Epoch: 5 | Loss: 0.6284360289573669 \n",
      "Epoch: 6 | Loss: 0.33979493379592896 \n",
      "Epoch: 7 | Loss: 0.2104371190071106 \n",
      "Epoch: 8 | Loss: 0.1520005166530609 \n",
      "Epoch: 9 | Loss: 0.12514783442020416 \n",
      "Epoch: 10 | Loss: 0.1123678907752037 \n",
      "Epoch: 11 | Loss: 0.1058642566204071 \n",
      "Epoch: 12 | Loss: 0.10216652601957321 \n",
      "Epoch: 13 | Loss: 0.09972938895225525 \n",
      "Epoch: 14 | Loss: 0.09786485135555267 \n",
      "Epoch: 15 | Loss: 0.09626659750938416 \n",
      "Epoch: 16 | Loss: 0.09479750692844391 \n",
      "Epoch: 17 | Loss: 0.09339704364538193 \n",
      "Epoch: 18 | Loss: 0.09203792363405228 \n",
      "Epoch: 19 | Loss: 0.09070750325918198 \n",
      "Epoch: 20 | Loss: 0.08940065652132034 \n",
      "Epoch: 21 | Loss: 0.08811423182487488 \n",
      "Epoch: 22 | Loss: 0.08684715628623962 \n",
      "Epoch: 23 | Loss: 0.0855989009141922 \n",
      "Epoch: 24 | Loss: 0.08436865359544754 \n",
      "Epoch: 25 | Loss: 0.08315599709749222 \n",
      "Epoch: 26 | Loss: 0.08196088671684265 \n",
      "Epoch: 27 | Loss: 0.08078287541866302 \n",
      "Epoch: 28 | Loss: 0.07962198555469513 \n",
      "Epoch: 29 | Loss: 0.0784776583313942 \n",
      "Epoch: 30 | Loss: 0.07734975218772888 \n",
      "Epoch: 31 | Loss: 0.07623820751905441 \n",
      "Epoch: 32 | Loss: 0.07514247298240662 \n",
      "Epoch: 33 | Loss: 0.0740625262260437 \n",
      "Epoch: 34 | Loss: 0.0729982778429985 \n",
      "Epoch: 35 | Loss: 0.07194908708333969 \n",
      "Epoch: 36 | Loss: 0.07091517001390457 \n",
      "Epoch: 37 | Loss: 0.06989588588476181 \n",
      "Epoch: 38 | Loss: 0.06889130175113678 \n",
      "Epoch: 39 | Loss: 0.06790123879909515 \n",
      "Epoch: 40 | Loss: 0.06692545115947723 \n",
      "Epoch: 41 | Loss: 0.06596371531486511 \n",
      "Epoch: 42 | Loss: 0.06501579284667969 \n",
      "Epoch: 43 | Loss: 0.06408136337995529 \n",
      "Epoch: 44 | Loss: 0.06316045671701431 \n",
      "Epoch: 45 | Loss: 0.062252696603536606 \n",
      "Epoch: 46 | Loss: 0.06135798245668411 \n",
      "Epoch: 47 | Loss: 0.06047617271542549 \n",
      "Epoch: 48 | Loss: 0.05960695818066597 \n",
      "Epoch: 49 | Loss: 0.05875040590763092 \n",
      "Epoch: 50 | Loss: 0.05790597200393677 \n",
      "Epoch: 51 | Loss: 0.057073887437582016 \n",
      "Epoch: 52 | Loss: 0.05625360459089279 \n",
      "Epoch: 53 | Loss: 0.05544508248567581 \n",
      "Epoch: 54 | Loss: 0.05464833602309227 \n",
      "Epoch: 55 | Loss: 0.05386286973953247 \n",
      "Epoch: 56 | Loss: 0.05308890715241432 \n",
      "Epoch: 57 | Loss: 0.0523257851600647 \n",
      "Epoch: 58 | Loss: 0.051573820412158966 \n",
      "Epoch: 59 | Loss: 0.05083272233605385 \n",
      "Epoch: 60 | Loss: 0.0501021072268486 \n",
      "Epoch: 61 | Loss: 0.04938201606273651 \n",
      "Epoch: 62 | Loss: 0.0486723855137825 \n",
      "Epoch: 63 | Loss: 0.047972869127988815 \n",
      "Epoch: 64 | Loss: 0.04728339612483978 \n",
      "Epoch: 65 | Loss: 0.046603888273239136 \n",
      "Epoch: 66 | Loss: 0.045934051275253296 \n",
      "Epoch: 67 | Loss: 0.04527396708726883 \n",
      "Epoch: 68 | Loss: 0.04462336748838425 \n",
      "Epoch: 69 | Loss: 0.04398198425769806 \n",
      "Epoch: 70 | Loss: 0.043349966406822205 \n",
      "Epoch: 71 | Loss: 0.042727015912532806 \n",
      "Epoch: 72 | Loss: 0.04211284592747688 \n",
      "Epoch: 73 | Loss: 0.04150766134262085 \n",
      "Epoch: 74 | Loss: 0.04091104120016098 \n",
      "Epoch: 75 | Loss: 0.04032304883003235 \n",
      "Epoch: 76 | Loss: 0.03974361717700958 \n",
      "Epoch: 77 | Loss: 0.03917253017425537 \n",
      "Epoch: 78 | Loss: 0.03860946744680405 \n",
      "Epoch: 79 | Loss: 0.03805467113852501 \n",
      "Epoch: 80 | Loss: 0.037507638335227966 \n",
      "Epoch: 81 | Loss: 0.036968592554330826 \n",
      "Epoch: 82 | Loss: 0.036437373608350754 \n",
      "Epoch: 83 | Loss: 0.035913679748773575 \n",
      "Epoch: 84 | Loss: 0.035397544503211975 \n",
      "Epoch: 85 | Loss: 0.03488876670598984 \n",
      "Epoch: 86 | Loss: 0.03438742831349373 \n",
      "Epoch: 87 | Loss: 0.033893197774887085 \n",
      "Epoch: 88 | Loss: 0.03340614214539528 \n",
      "Epoch: 89 | Loss: 0.03292606398463249 \n",
      "Epoch: 90 | Loss: 0.03245282545685768 \n",
      "Epoch: 91 | Loss: 0.03198644146323204 \n",
      "Epoch: 92 | Loss: 0.03152673691511154 \n",
      "Epoch: 93 | Loss: 0.031073659658432007 \n",
      "Epoch: 94 | Loss: 0.03062707744538784 \n",
      "Epoch: 95 | Loss: 0.030186869204044342 \n",
      "Epoch: 96 | Loss: 0.029753098264336586 \n",
      "Epoch: 97 | Loss: 0.02932550013065338 \n",
      "Epoch: 98 | Loss: 0.028904085978865623 \n",
      "Epoch: 99 | Loss: 0.028488673269748688 \n",
      "Epoch: 100 | Loss: 0.028079161420464516 \n",
      "Epoch: 101 | Loss: 0.027675671502947807 \n",
      "Epoch: 102 | Loss: 0.02727789431810379 \n",
      "Epoch: 103 | Loss: 0.026885859668254852 \n",
      "Epoch: 104 | Loss: 0.026499535888433456 \n",
      "Epoch: 105 | Loss: 0.0261186882853508 \n",
      "Epoch: 106 | Loss: 0.025743301957845688 \n",
      "Epoch: 107 | Loss: 0.025373294949531555 \n",
      "Epoch: 108 | Loss: 0.025008674710989 \n",
      "Epoch: 109 | Loss: 0.024649247527122498 \n",
      "Epoch: 110 | Loss: 0.024294983595609665 \n",
      "Epoch: 111 | Loss: 0.023945845663547516 \n",
      "Epoch: 112 | Loss: 0.02360168844461441 \n",
      "Epoch: 113 | Loss: 0.02326248213648796 \n",
      "Epoch: 114 | Loss: 0.022928159683942795 \n",
      "Epoch: 115 | Loss: 0.022598695009946823 \n",
      "Epoch: 116 | Loss: 0.02227393165230751 \n",
      "Epoch: 117 | Loss: 0.02195378765463829 \n",
      "Epoch: 118 | Loss: 0.021638257429003716 \n",
      "Epoch: 119 | Loss: 0.021327348425984383 \n",
      "Epoch: 120 | Loss: 0.021020831540226936 \n",
      "Epoch: 121 | Loss: 0.0207186508923769 \n",
      "Epoch: 122 | Loss: 0.020420964807271957 \n",
      "Epoch: 123 | Loss: 0.02012740634381771 \n",
      "Epoch: 124 | Loss: 0.019838184118270874 \n",
      "Epoch: 125 | Loss: 0.01955312117934227 \n",
      "Epoch: 126 | Loss: 0.01927211321890354 \n",
      "Epoch: 127 | Loss: 0.018995095044374466 \n",
      "Epoch: 128 | Loss: 0.0187221746891737 \n",
      "Epoch: 129 | Loss: 0.018452998250722885 \n",
      "Epoch: 130 | Loss: 0.01818789727985859 \n",
      "Epoch: 131 | Loss: 0.01792643405497074 \n",
      "Epoch: 132 | Loss: 0.017668843269348145 \n",
      "Epoch: 133 | Loss: 0.017414871603250504 \n",
      "Epoch: 134 | Loss: 0.017164628952741623 \n",
      "Epoch: 135 | Loss: 0.016917897388339043 \n",
      "Epoch: 136 | Loss: 0.016674809157848358 \n",
      "Epoch: 137 | Loss: 0.01643514819443226 \n",
      "Epoch: 138 | Loss: 0.016198931261897087 \n",
      "Epoch: 139 | Loss: 0.015966180711984634 \n",
      "Epoch: 140 | Loss: 0.015736697241663933 \n",
      "Epoch: 141 | Loss: 0.01551056932657957 \n",
      "Epoch: 142 | Loss: 0.015287674963474274 \n",
      "Epoch: 143 | Loss: 0.015067927539348602 \n",
      "Epoch: 144 | Loss: 0.014851321466267109 \n",
      "Epoch: 145 | Loss: 0.014637903310358524 \n",
      "Epoch: 146 | Loss: 0.014427557587623596 \n",
      "Epoch: 147 | Loss: 0.014220220036804676 \n",
      "Epoch: 148 | Loss: 0.01401587575674057 \n",
      "Epoch: 149 | Loss: 0.013814466074109077 \n",
      "Epoch: 150 | Loss: 0.013615909963846207 \n",
      "Epoch: 151 | Loss: 0.013420208357274532 \n",
      "Epoch: 152 | Loss: 0.013227352872490883 \n",
      "Epoch: 153 | Loss: 0.013037208467721939 \n",
      "Epoch: 154 | Loss: 0.012849817983806133 \n",
      "Epoch: 155 | Loss: 0.01266518235206604 \n",
      "Epoch: 156 | Loss: 0.012483152560889721 \n",
      "Epoch: 157 | Loss: 0.012303742580115795 \n",
      "Epoch: 158 | Loss: 0.012126930058002472 \n",
      "Epoch: 159 | Loss: 0.011952681466937065 \n",
      "Epoch: 160 | Loss: 0.011780919507145882 \n",
      "Epoch: 161 | Loss: 0.011611607857048512 \n",
      "Epoch: 162 | Loss: 0.011444692499935627 \n",
      "Epoch: 163 | Loss: 0.011280270293354988 \n",
      "Epoch: 164 | Loss: 0.011118097230792046 \n",
      "Epoch: 165 | Loss: 0.01095832884311676 \n",
      "Epoch: 166 | Loss: 0.010800839401781559 \n",
      "Epoch: 167 | Loss: 0.010645613074302673 \n",
      "Epoch: 168 | Loss: 0.01049262285232544 \n",
      "Epoch: 169 | Loss: 0.010341846384108067 \n",
      "Epoch: 170 | Loss: 0.01019315980374813 \n",
      "Epoch: 171 | Loss: 0.01004670187830925 \n",
      "Epoch: 172 | Loss: 0.009902330115437508 \n",
      "Epoch: 173 | Loss: 0.009760024026036263 \n",
      "Epoch: 174 | Loss: 0.00961974635720253 \n",
      "Epoch: 175 | Loss: 0.00948144681751728 \n",
      "Epoch: 176 | Loss: 0.009345199912786484 \n",
      "Epoch: 177 | Loss: 0.009210880845785141 \n",
      "Epoch: 178 | Loss: 0.009078560397028923 \n",
      "Epoch: 179 | Loss: 0.008948043920099735 \n",
      "Epoch: 180 | Loss: 0.008819438517093658 \n",
      "Epoch: 181 | Loss: 0.00869271345436573 \n",
      "Epoch: 182 | Loss: 0.008567793294787407 \n",
      "Epoch: 183 | Loss: 0.008444679901003838 \n",
      "Epoch: 184 | Loss: 0.00832330621778965 \n",
      "Epoch: 185 | Loss: 0.008203663863241673 \n",
      "Epoch: 186 | Loss: 0.008085797540843487 \n",
      "Epoch: 187 | Loss: 0.007969594560563564 \n",
      "Epoch: 188 | Loss: 0.007855040952563286 \n",
      "Epoch: 189 | Loss: 0.007742151152342558 \n",
      "Epoch: 190 | Loss: 0.007630875799804926 \n",
      "Epoch: 191 | Loss: 0.0075212023220956326 \n",
      "Epoch: 192 | Loss: 0.007413153536617756 \n",
      "Epoch: 193 | Loss: 0.007306577637791634 \n",
      "Epoch: 194 | Loss: 0.007201563101261854 \n",
      "Epoch: 195 | Loss: 0.007098058238625526 \n",
      "Epoch: 196 | Loss: 0.006996080279350281 \n",
      "Epoch: 197 | Loss: 0.0068955207243561745 \n",
      "Epoch: 198 | Loss: 0.006796442903578281 \n",
      "Epoch: 199 | Loss: 0.006698785349726677 \n",
      "Epoch: 200 | Loss: 0.0066024865955114365 \n",
      "Epoch: 201 | Loss: 0.0065076351165771484 \n",
      "Epoch: 202 | Loss: 0.006414076313376427 \n",
      "Epoch: 203 | Loss: 0.0063218832947313786 \n",
      "Epoch: 204 | Loss: 0.0062310462817549706 \n",
      "Epoch: 205 | Loss: 0.0061415089294314384 \n",
      "Epoch: 206 | Loss: 0.006053230259567499 \n",
      "Epoch: 207 | Loss: 0.0059662191197276115 \n",
      "Epoch: 208 | Loss: 0.005880516488105059 \n",
      "Epoch: 209 | Loss: 0.005795986391603947 \n",
      "Epoch: 210 | Loss: 0.005712705664336681 \n",
      "Epoch: 211 | Loss: 0.005630564410239458 \n",
      "Epoch: 212 | Loss: 0.005549641326069832 \n",
      "Epoch: 213 | Loss: 0.005469923373311758 \n",
      "Epoch: 214 | Loss: 0.0053913164883852005 \n",
      "Epoch: 215 | Loss: 0.005313841626048088 \n",
      "Epoch: 216 | Loss: 0.0052374363876879215 \n",
      "Epoch: 217 | Loss: 0.0051621864549815655 \n",
      "Epoch: 218 | Loss: 0.005087999626994133 \n",
      "Epoch: 219 | Loss: 0.005014847498387098 \n",
      "Epoch: 220 | Loss: 0.004942793399095535 \n",
      "Epoch: 221 | Loss: 0.004871752113103867 \n",
      "Epoch: 222 | Loss: 0.004801740404218435 \n",
      "Epoch: 223 | Loss: 0.0047327433712780476 \n",
      "Epoch: 224 | Loss: 0.004664710722863674 \n",
      "Epoch: 225 | Loss: 0.004597655031830072 \n",
      "Epoch: 226 | Loss: 0.004531605169177055 \n",
      "Epoch: 227 | Loss: 0.004466473124921322 \n",
      "Epoch: 228 | Loss: 0.004402259364724159 \n",
      "Epoch: 229 | Loss: 0.004339008592069149 \n",
      "Epoch: 230 | Loss: 0.004276645835489035 \n",
      "Epoch: 231 | Loss: 0.004215172026306391 \n",
      "Epoch: 232 | Loss: 0.004154637455940247 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 233 | Loss: 0.004094916861504316 \n",
      "Epoch: 234 | Loss: 0.004036046098917723 \n",
      "Epoch: 235 | Loss: 0.003978070802986622 \n",
      "Epoch: 236 | Loss: 0.003920861054211855 \n",
      "Epoch: 237 | Loss: 0.003864553291350603 \n",
      "Epoch: 238 | Loss: 0.0038090047892183065 \n",
      "Epoch: 239 | Loss: 0.0037542572245001793 \n",
      "Epoch: 240 | Loss: 0.003700309433043003 \n",
      "Epoch: 241 | Loss: 0.0036471099592745304 \n",
      "Epoch: 242 | Loss: 0.003594717476516962 \n",
      "Epoch: 243 | Loss: 0.0035430295392870903 \n",
      "Epoch: 244 | Loss: 0.0034920903854072094 \n",
      "Epoch: 245 | Loss: 0.0034419463481754065 \n",
      "Epoch: 246 | Loss: 0.003392463084310293 \n",
      "Epoch: 247 | Loss: 0.0033437246456742287 \n",
      "Epoch: 248 | Loss: 0.0032956362701952457 \n",
      "Epoch: 249 | Loss: 0.0032482901588082314 \n",
      "Epoch: 250 | Loss: 0.003201605984941125 \n",
      "Epoch: 251 | Loss: 0.003155576530843973 \n",
      "Epoch: 252 | Loss: 0.003110240213572979 \n",
      "Epoch: 253 | Loss: 0.0030655215959995985 \n",
      "Epoch: 254 | Loss: 0.00302149448543787 \n",
      "Epoch: 255 | Loss: 0.002978065051138401 \n",
      "Epoch: 256 | Loss: 0.0029352493584156036 \n",
      "Epoch: 257 | Loss: 0.002893052762374282 \n",
      "Epoch: 258 | Loss: 0.0028514883015304804 \n",
      "Epoch: 259 | Loss: 0.0028104986995458603 \n",
      "Epoch: 260 | Loss: 0.0027701272629201412 \n",
      "Epoch: 261 | Loss: 0.0027303211390972137 \n",
      "Epoch: 262 | Loss: 0.002691077534109354 \n",
      "Epoch: 263 | Loss: 0.0026524027343839407 \n",
      "Epoch: 264 | Loss: 0.002614296041429043 \n",
      "Epoch: 265 | Loss: 0.0025767041370272636 \n",
      "Epoch: 266 | Loss: 0.0025396952405571938 \n",
      "Epoch: 267 | Loss: 0.0025031983386725187 \n",
      "Epoch: 268 | Loss: 0.0024672255385667086 \n",
      "Epoch: 269 | Loss: 0.0024317624047398567 \n",
      "Epoch: 270 | Loss: 0.0023968182504177094 \n",
      "Epoch: 271 | Loss: 0.0023623525630682707 \n",
      "Epoch: 272 | Loss: 0.0023283865302801132 \n",
      "Epoch: 273 | Loss: 0.0022949392441660166 \n",
      "Epoch: 274 | Loss: 0.0022619415540248156 \n",
      "Epoch: 275 | Loss: 0.0022294488735497 \n",
      "Epoch: 276 | Loss: 0.00219742301851511 \n",
      "Epoch: 277 | Loss: 0.002165823942050338 \n",
      "Epoch: 278 | Loss: 0.0021346951834857464 \n",
      "Epoch: 279 | Loss: 0.002104031853377819 \n",
      "Epoch: 280 | Loss: 0.0020737862214446068 \n",
      "Epoch: 281 | Loss: 0.002043959451839328 \n",
      "Epoch: 282 | Loss: 0.002014614874497056 \n",
      "Epoch: 283 | Loss: 0.0019856432918459177 \n",
      "Epoch: 284 | Loss: 0.0019571282900869846 \n",
      "Epoch: 285 | Loss: 0.0019289982737973332 \n",
      "Epoch: 286 | Loss: 0.0019012519624084234 \n",
      "Epoch: 287 | Loss: 0.001873923116363585 \n",
      "Epoch: 288 | Loss: 0.0018470119684934616 \n",
      "Epoch: 289 | Loss: 0.001820454141125083 \n",
      "Epoch: 290 | Loss: 0.001794300740584731 \n",
      "Epoch: 291 | Loss: 0.0017685319762676954 \n",
      "Epoch: 292 | Loss: 0.001743100001476705 \n",
      "Epoch: 293 | Loss: 0.0017180645372718573 \n",
      "Epoch: 294 | Loss: 0.001693358295597136 \n",
      "Epoch: 295 | Loss: 0.0016690235352143645 \n",
      "Epoch: 296 | Loss: 0.0016450535040348768 \n",
      "Epoch: 297 | Loss: 0.0016214019851759076 \n",
      "Epoch: 298 | Loss: 0.0015980752650648355 \n",
      "Epoch: 299 | Loss: 0.0015751321334391832 \n",
      "Epoch: 300 | Loss: 0.0015524853952229023 \n",
      "Epoch: 301 | Loss: 0.001530168578028679 \n",
      "Epoch: 302 | Loss: 0.001508189714513719 \n",
      "Epoch: 303 | Loss: 0.0014865199336782098 \n",
      "Epoch: 304 | Loss: 0.0014651359524577856 \n",
      "Epoch: 305 | Loss: 0.001444085966795683 \n",
      "Epoch: 306 | Loss: 0.0014233418041840196 \n",
      "Epoch: 307 | Loss: 0.0014028784353286028 \n",
      "Epoch: 308 | Loss: 0.0013827059883624315 \n",
      "Epoch: 309 | Loss: 0.0013628576416522264 \n",
      "Epoch: 310 | Loss: 0.0013432487612590194 \n",
      "Epoch: 311 | Loss: 0.0013239511754363775 \n",
      "Epoch: 312 | Loss: 0.0013049213448539376 \n",
      "Epoch: 313 | Loss: 0.0012861710274592042 \n",
      "Epoch: 314 | Loss: 0.0012676973128691316 \n",
      "Epoch: 315 | Loss: 0.0012494763359427452 \n",
      "Epoch: 316 | Loss: 0.0012315077474340796 \n",
      "Epoch: 317 | Loss: 0.001213821000419557 \n",
      "Epoch: 318 | Loss: 0.0011963871074840426 \n",
      "Epoch: 319 | Loss: 0.0011791650904342532 \n",
      "Epoch: 320 | Loss: 0.0011622185120359063 \n",
      "Epoch: 321 | Loss: 0.0011455286294221878 \n",
      "Epoch: 322 | Loss: 0.0011290725087746978 \n",
      "Epoch: 323 | Loss: 0.0011128383921459317 \n",
      "Epoch: 324 | Loss: 0.0010968470014631748 \n",
      "Epoch: 325 | Loss: 0.0010810650419443846 \n",
      "Epoch: 326 | Loss: 0.0010655359365046024 \n",
      "Epoch: 327 | Loss: 0.0010502284858375788 \n",
      "Epoch: 328 | Loss: 0.0010351496748626232 \n",
      "Epoch: 329 | Loss: 0.001020269002765417 \n",
      "Epoch: 330 | Loss: 0.0010055992752313614 \n",
      "Epoch: 331 | Loss: 0.0009911565575748682 \n",
      "Epoch: 332 | Loss: 0.0009768966119736433 \n",
      "Epoch: 333 | Loss: 0.0009628679836168885 \n",
      "Epoch: 334 | Loss: 0.0009490161319263279 \n",
      "Epoch: 335 | Loss: 0.0009353872155770659 \n",
      "Epoch: 336 | Loss: 0.0009219346102327108 \n",
      "Epoch: 337 | Loss: 0.0009086846839636564 \n",
      "Epoch: 338 | Loss: 0.0008956314995884895 \n",
      "Epoch: 339 | Loss: 0.0008827522397041321 \n",
      "Epoch: 340 | Loss: 0.000870084622874856 \n",
      "Epoch: 341 | Loss: 0.0008575681131333113 \n",
      "Epoch: 342 | Loss: 0.0008452454349026084 \n",
      "Epoch: 343 | Loss: 0.0008330974960699677 \n",
      "Epoch: 344 | Loss: 0.0008211141685023904 \n",
      "Epoch: 345 | Loss: 0.0008093316573649645 \n",
      "Epoch: 346 | Loss: 0.0007976919296197593 \n",
      "Epoch: 347 | Loss: 0.0007862359052523971 \n",
      "Epoch: 348 | Loss: 0.0007749194046482444 \n",
      "Epoch: 349 | Loss: 0.0007637973758392036 \n",
      "Epoch: 350 | Loss: 0.0007528276182711124 \n",
      "Epoch: 351 | Loss: 0.000742002041079104 \n",
      "Epoch: 352 | Loss: 0.0007313303649425507 \n",
      "Epoch: 353 | Loss: 0.0007208172464743257 \n",
      "Epoch: 354 | Loss: 0.0007104746764525771 \n",
      "Epoch: 355 | Loss: 0.0007002499769441783 \n",
      "Epoch: 356 | Loss: 0.0006901894812472165 \n",
      "Epoch: 357 | Loss: 0.0006802700227126479 \n",
      "Epoch: 358 | Loss: 0.0006704917177557945 \n",
      "Epoch: 359 | Loss: 0.0006608618423342705 \n",
      "Epoch: 360 | Loss: 0.0006513698608614504 \n",
      "Epoch: 361 | Loss: 0.0006419908022508025 \n",
      "Epoch: 362 | Loss: 0.000632785027846694 \n",
      "Epoch: 363 | Loss: 0.0006236820481717587 \n",
      "Epoch: 364 | Loss: 0.0006147199892438948 \n",
      "Epoch: 365 | Loss: 0.000605885754339397 \n",
      "Epoch: 366 | Loss: 0.0005971680511720479 \n",
      "Epoch: 367 | Loss: 0.000588598137255758 \n",
      "Epoch: 368 | Loss: 0.0005801335792057216 \n",
      "Epoch: 369 | Loss: 0.0005717900930903852 \n",
      "Epoch: 370 | Loss: 0.000563582987524569 \n",
      "Epoch: 371 | Loss: 0.0005554722156375647 \n",
      "Epoch: 372 | Loss: 0.0005474950303323567 \n",
      "Epoch: 373 | Loss: 0.0005396335618570447 \n",
      "Epoch: 374 | Loss: 0.0005318666226230562 \n",
      "Epoch: 375 | Loss: 0.0005242318147793412 \n",
      "Epoch: 376 | Loss: 0.0005166865303181112 \n",
      "Epoch: 377 | Loss: 0.0005092667997814715 \n",
      "Epoch: 378 | Loss: 0.0005019502714276314 \n",
      "Epoch: 379 | Loss: 0.0004947345005348325 \n",
      "Epoch: 380 | Loss: 0.00048762623919174075 \n",
      "Epoch: 381 | Loss: 0.0004806203069165349 \n",
      "Epoch: 382 | Loss: 0.00047370942775160074 \n",
      "Epoch: 383 | Loss: 0.0004669076588470489 \n",
      "Epoch: 384 | Loss: 0.0004601899709086865 \n",
      "Epoch: 385 | Loss: 0.0004535754560492933 \n",
      "Epoch: 386 | Loss: 0.0004470567509997636 \n",
      "Epoch: 387 | Loss: 0.0004406328371260315 \n",
      "Epoch: 388 | Loss: 0.00043430872028693557 \n",
      "Epoch: 389 | Loss: 0.0004280556458979845 \n",
      "Epoch: 390 | Loss: 0.00042190871317870915 \n",
      "Epoch: 391 | Loss: 0.0004158442607149482 \n",
      "Epoch: 392 | Loss: 0.00040986842941492796 \n",
      "Epoch: 393 | Loss: 0.00040397903649136424 \n",
      "Epoch: 394 | Loss: 0.0003981740737799555 \n",
      "Epoch: 395 | Loss: 0.00039245604421012104 \n",
      "Epoch: 396 | Loss: 0.0003868033818434924 \n",
      "Epoch: 397 | Loss: 0.00038124507409520447 \n",
      "Epoch: 398 | Loss: 0.0003757675876840949 \n",
      "Epoch: 399 | Loss: 0.0003703621623571962 \n",
      "Epoch: 400 | Loss: 0.00036505365278571844 \n",
      "Epoch: 401 | Loss: 0.0003598011389840394 \n",
      "Epoch: 402 | Loss: 0.00035463704261928797 \n",
      "Epoch: 403 | Loss: 0.00034954180591739714 \n",
      "Epoch: 404 | Loss: 0.000344514730386436 \n",
      "Epoch: 405 | Loss: 0.000339555146638304 \n",
      "Epoch: 406 | Loss: 0.00033468895708210766 \n",
      "Epoch: 407 | Loss: 0.00032986741280183196 \n",
      "Epoch: 408 | Loss: 0.00032512377947568893 \n",
      "Epoch: 409 | Loss: 0.0003204561653546989 \n",
      "Epoch: 410 | Loss: 0.0003158595063723624 \n",
      "Epoch: 411 | Loss: 0.0003113124694209546 \n",
      "Epoch: 412 | Loss: 0.0003068462829105556 \n",
      "Epoch: 413 | Loss: 0.0003024234902113676 \n",
      "Epoch: 414 | Loss: 0.0002980880090035498 \n",
      "Epoch: 415 | Loss: 0.00029379676561802626 \n",
      "Epoch: 416 | Loss: 0.0002895831421483308 \n",
      "Epoch: 417 | Loss: 0.00028541075880639255 \n",
      "Epoch: 418 | Loss: 0.0002813223109114915 \n",
      "Epoch: 419 | Loss: 0.00027727108681574464 \n",
      "Epoch: 420 | Loss: 0.0002732848224695772 \n",
      "Epoch: 421 | Loss: 0.0002693550777621567 \n",
      "Epoch: 422 | Loss: 0.0002654916897881776 \n",
      "Epoch: 423 | Loss: 0.0002616684651002288 \n",
      "Epoch: 424 | Loss: 0.00025790746440179646 \n",
      "Epoch: 425 | Loss: 0.0002542080183047801 \n",
      "Epoch: 426 | Loss: 0.00025054911384359 \n",
      "Epoch: 427 | Loss: 0.0002469514438416809 \n",
      "Epoch: 428 | Loss: 0.00024339431547559798 \n",
      "Epoch: 429 | Loss: 0.0002399008080828935 \n",
      "Epoch: 430 | Loss: 0.0002364539832342416 \n",
      "Epoch: 431 | Loss: 0.00023306056391447783 \n",
      "Epoch: 432 | Loss: 0.00022970748250372708 \n",
      "Epoch: 433 | Loss: 0.00022640934912487864 \n",
      "Epoch: 434 | Loss: 0.00022315848036669195 \n",
      "Epoch: 435 | Loss: 0.00021994237613398582 \n",
      "Epoch: 436 | Loss: 0.00021678382472600788 \n",
      "Epoch: 437 | Loss: 0.00021367277076933533 \n",
      "Epoch: 438 | Loss: 0.000210598562262021 \n",
      "Epoch: 439 | Loss: 0.0002075767843052745 \n",
      "Epoch: 440 | Loss: 0.00020458849030546844 \n",
      "Epoch: 441 | Loss: 0.0002016548824030906 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 442 | Loss: 0.00019875145517289639 \n",
      "Epoch: 443 | Loss: 0.00019589514704421163 \n",
      "Epoch: 444 | Loss: 0.00019307727052364498 \n",
      "Epoch: 445 | Loss: 0.00019030469411518425 \n",
      "Epoch: 446 | Loss: 0.00018756809004116803 \n",
      "Epoch: 447 | Loss: 0.00018487263878341764 \n",
      "Epoch: 448 | Loss: 0.00018221710342913866 \n",
      "Epoch: 449 | Loss: 0.0001795994903659448 \n",
      "Epoch: 450 | Loss: 0.0001770162780303508 \n",
      "Epoch: 451 | Loss: 0.00017447560094296932 \n",
      "Epoch: 452 | Loss: 0.00017196623957715929 \n",
      "Epoch: 453 | Loss: 0.00016949549899436533 \n",
      "Epoch: 454 | Loss: 0.000167060672538355 \n",
      "Epoch: 455 | Loss: 0.00016465986846014857 \n",
      "Epoch: 456 | Loss: 0.00016228831373155117 \n",
      "Epoch: 457 | Loss: 0.00015995599096640944 \n",
      "Epoch: 458 | Loss: 0.00015766246360726655 \n",
      "Epoch: 459 | Loss: 0.0001553949259687215 \n",
      "Epoch: 460 | Loss: 0.00015315963537432253 \n",
      "Epoch: 461 | Loss: 0.00015095841081347317 \n",
      "Epoch: 462 | Loss: 0.00014878800720907748 \n",
      "Epoch: 463 | Loss: 0.0001466488029109314 \n",
      "Epoch: 464 | Loss: 0.00014454606571234763 \n",
      "Epoch: 465 | Loss: 0.00014246476348489523 \n",
      "Epoch: 466 | Loss: 0.0001404185313731432 \n",
      "Epoch: 467 | Loss: 0.00013839667371939868 \n",
      "Epoch: 468 | Loss: 0.00013641189434565604 \n",
      "Epoch: 469 | Loss: 0.0001344535849057138 \n",
      "Epoch: 470 | Loss: 0.00013252282224129885 \n",
      "Epoch: 471 | Loss: 0.0001306086778640747 \n",
      "Epoch: 472 | Loss: 0.00012873814557678998 \n",
      "Epoch: 473 | Loss: 0.00012688504648394883 \n",
      "Epoch: 474 | Loss: 0.00012506681377999485 \n",
      "Epoch: 475 | Loss: 0.00012326559226494282 \n",
      "Epoch: 476 | Loss: 0.00012149665417382494 \n",
      "Epoch: 477 | Loss: 0.00011974433436989784 \n",
      "Epoch: 478 | Loss: 0.00011802493827417493 \n",
      "Epoch: 479 | Loss: 0.00011633178655756637 \n",
      "Epoch: 480 | Loss: 0.000114657093945425 \n",
      "Epoch: 481 | Loss: 0.00011300628102617338 \n",
      "Epoch: 482 | Loss: 0.00011138523404952139 \n",
      "Epoch: 483 | Loss: 0.00010978685895679519 \n",
      "Epoch: 484 | Loss: 0.00010820850729942322 \n",
      "Epoch: 485 | Loss: 0.00010665420268196613 \n",
      "Epoch: 486 | Loss: 0.00010512006701901555 \n",
      "Epoch: 487 | Loss: 0.00010361127351643518 \n",
      "Epoch: 488 | Loss: 0.00010212218330707401 \n",
      "Epoch: 489 | Loss: 0.00010065670358017087 \n",
      "Epoch: 490 | Loss: 9.920529555529356e-05 \n",
      "Epoch: 491 | Loss: 9.777765808394179e-05 \n",
      "Epoch: 492 | Loss: 9.637405310058966e-05 \n",
      "Epoch: 493 | Loss: 9.498909639660269e-05 \n",
      "Epoch: 494 | Loss: 9.362486161990091e-05 \n",
      "Epoch: 495 | Loss: 9.228110866388306e-05 \n",
      "Epoch: 496 | Loss: 9.095371206058189e-05 \n",
      "Epoch: 497 | Loss: 8.964531298261136e-05 \n",
      "Epoch: 498 | Loss: 8.835893822833896e-05 \n",
      "Epoch: 499 | Loss: 8.708512905286625e-05 \n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # 2) Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (after training) 4 7.989271640777588\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "hour_var = tensor([[4.0]])\n",
    "y_pred = model(hour_var)\n",
    "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[1.9938]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0141], requires_grad=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
